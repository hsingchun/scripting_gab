{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import bs4\n",
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from datetime import date\n",
    "import glob\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_repeat_watch_days = 10\n",
    "n_post_target = 50\n",
    "EMAIL = 'cattaiangela2018@gmail.com'\n",
    "PASSWORD = 'dontbotherme'\n",
    "\n",
    "def cal_engagement(replies, reblogs, likes, followers):\n",
    "    eng_rate = (likes+replies*3+reblogs*2)/followers\n",
    "    return eng_rate\n",
    "\n",
    "date_format = '%Y-%m-%d'\n",
    "today = date.today()\n",
    "date_str = '_'+str(today).replace('-','')\n",
    "user_agent = [\n",
    "    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.114 Safari/537.36\",\n",
    "    \"Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 6.0; Acoo Browser; SLCC1; .NET CLR 2.0.50727; Media Center PC 5.0; .NET CLR 3.0.04506)\",\n",
    "    \"Mozilla/4.0 (compatible; MSIE 7.0; AOL 9.5; AOLBuild 4337.35; Windows NT 5.1; .NET CLR 1.1.4322; .NET CLR 2.0.50727)\",\n",
    "    \"Mozilla/5.0 (Windows; U; MSIE 9.0; Windows NT 9.0; en-US)\",\n",
    "    \"Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; Win64; x64; Trident/5.0; .NET CLR 3.5.30729; .NET CLR 3.0.30729; .NET CLR 2.0.50727; Media Center PC 6.0)\",\n",
    "    \"Mozilla/5.0 (compatible; MSIE 8.0; Windows NT 6.0; Trident/4.0; WOW64; Trident/4.0; SLCC2; .NET CLR 2.0.50727; .NET CLR 3.5.30729; .NET CLR 3.0.30729; .NET CLR 1.0.3705; .NET CLR 1.1.4322)\",\n",
    "    \"Mozilla/4.0 (compatible; MSIE 7.0b; Windows NT 5.2; .NET CLR 1.1.4322; .NET CLR 2.0.50727; InfoPath.2; .NET CLR 3.0.04506.30)\",\n",
    "    \"Mozilla/5.0 (Windows; U; Windows NT 5.1; zh-CN) AppleWebKit/523.15 (KHTML, like Gecko, Safari/419.3) Arora/0.3 (Change: 287 c9dfb30)\",\n",
    "    \"Mozilla/5.0 (X11; U; Linux; en-US) AppleWebKit/527+ (KHTML, like Gecko, Safari/419.3) Arora/0.6\",\n",
    "    \"Mozilla/5.0 (Windows; U; Windows NT 5.1; en-US; rv:1.8.1.2pre) Gecko/20070215 K-Ninja/2.1.1\",\n",
    "    \"Mozilla/5.0 (Windows; U; Windows NT 5.1; zh-CN; rv:1.9) Gecko/20080705 Firefox/3.0 Kapiko/3.0\",\n",
    "    \"Mozilla/5.0 (X11; Linux i686; U;) Gecko/20070322 Kazehakase/0.4.5\",\n",
    "    \"Mozilla/5.0 (X11; U; Linux i686; en-US; rv:1.9.0.8) Gecko Fedora/1.9.0.8-1.fc10 Kazehakase/0.5.6\",\n",
    "    \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/535.11 (KHTML, like Gecko) Chrome/17.0.963.56 Safari/535.11\",\n",
    "    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_7_3) AppleWebKit/535.20 (KHTML, like Gecko) Chrome/19.0.1036.7 Safari/535.20\"\n",
    "]\n",
    "\n",
    "url_base = 'https://gab.com/'\n",
    "ua = random.choice(user_agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_json(dict_all, file_name):\n",
    "    path = 'user_data/'\n",
    "    with open(path+file_name+'.json', 'w') as f:\n",
    "        json.dump(dict_all, f)\n",
    "def get_basic_info(res_post):\n",
    "    post = res_post[0]\n",
    "    dict_user = dict()\n",
    "    dict_user['user_id'] = post['account']['id']\n",
    "    dict_user['date_joined'] = post['account']['created_at']\n",
    "    dict_user['user_name'] = post['account']['username']\n",
    "    dict_user['gabs'] = post['account']['statuses_count']\n",
    "    dict_user['followers'] = post['account']['followers_count']\n",
    "    dict_user['following'] = post['account']['following_count']\n",
    "    return dict_user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_n_posts(dict_user, n_post_target,user_id, rs, headers):\n",
    "    n_post = len(dict_user['last_50_posts'])\n",
    "    last_id = dict_user['last_50_posts'][-1]['post_id']\n",
    "\n",
    "    url_base = 'https://gab.com/'\n",
    "    while (n_post < n_post_target):\n",
    "        try:\n",
    "            time.sleep(random.randint(3,10))\n",
    "            uri = 'api/v1/accounts/'+user_id+'/statuses?exclude_replies=true&max_id='+last_id\n",
    "            url2 = url_base+uri\n",
    "            response = rs.get(url2, headers = headers)\n",
    "            res = json.loads(response.text)\n",
    "        except Exception as e:\n",
    "            print('user_id = ', user_id)\n",
    "            print('[get_n_posts] Failed when n_post = ', n_post)\n",
    "            print(e)\n",
    "        if (len(response.text) < 5)|(len(res) < 5):\n",
    "            break\n",
    "        else:\n",
    "            last_id = res[-1]['id']\n",
    "            res_posts = [get_post_dict(post) for post in res]\n",
    "            dict_user['last_50_posts'].extend(res_posts)\n",
    "            n_post = len(dict_user['last_50_posts'])\n",
    "    return dict_user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_post_dict(post):\n",
    "    dict_post = dict()\n",
    "    dict_post['post_id'] = post['id']\n",
    "    if post['reblog'] is None:\n",
    "        dict_post['is_repost'] = 'no'\n",
    "        dict_post['replies_count'] = post['replies_count']\n",
    "        dict_post['reblogs_count'] = post['reblogs_count']\n",
    "        dict_post['favourites_count'] = post['favourites_count']\n",
    "        dict_post['content'] = post['content']\n",
    "    else:\n",
    "        dict_post['is_repost'] = 'yes'\n",
    "        dict_post['replies_count'] = post['reblog']['replies_count']\n",
    "        dict_post['reblogs_count'] = post['reblog']['reblogs_count']\n",
    "        dict_post['favourites_count'] = post['reblog']['favourites_count']\n",
    "        dict_post['reblog_content'] = post['reblog']['content']\n",
    "        dict_post['reblog_account'] = post['reblog']['account']\n",
    "        dict_post['reblog_media_attachments'] = post['reblog']['media_attachments']\n",
    "    dict_post['media_attachments'] = post['media_attachments']\n",
    "    dict_post['tags'] = post['tags']\n",
    "    return dict_post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_data(user_id,rs,headers):\n",
    "    url = 'https://gab.com/api/v1/accounts/'+user_id+'/statuses?exclude_replies=true'\n",
    "    html = rs.get(url, headers=headers)\n",
    "    obj = bs4.BeautifulSoup(html.text, 'lxml')\n",
    "    text = obj.select('p')[0].text\n",
    "    res_post = json.loads(text)\n",
    "    # get user data and posts on first page\n",
    "    dict_user = get_basic_info(res_post)\n",
    "    last_50_posts = [get_post_dict(post) for post in res_post]\n",
    "    dict_user['last_50_posts'] = last_50_posts\n",
    "    \n",
    "    # get rest posts\n",
    "    dict_user = get_n_posts(dict_user, n_post_target,user_id, rs, headers)\n",
    "    # avg_engagement\n",
    "    f = dict_user['followers']\n",
    "    eng_rates = [cal_engagement(x['replies_count'], x['reblogs_count'], x['favourites_count'],f) for x in dict_user['last_50_posts'] if x['is_repost'] == 'no']\n",
    "    dict_user['avg_engagement'] = np.mean(eng_rates)\n",
    "    write_json(dict_user, user_id)\n",
    "    return dict_user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_explore(response):\n",
    "    obj = bs4.BeautifulSoup(response.text, 'lxml')\n",
    "    text = obj.select('p')[0].text\n",
    "    explore_post = json.loads(text)\n",
    "    return explore_post"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# login page & get token\n",
    "LOGIN_URL = 'https://gab.com/auth/sign_in'\n",
    "rs = requests.session()\n",
    "\n",
    "result = rs.get(LOGIN_URL)\n",
    "obj = bs4.BeautifulSoup(result.text, 'lxml')\n",
    "token = str(obj.find('input',attrs={'name':\"authenticity_token\"}))\n",
    "authenticity_token = token.split('\"')[-2]\n",
    "cookie = rs.cookies.get_dict()\n",
    "cookie = list(cookie.keys())[0]+'='+list(cookie.values())[0]+';'+list(cookie.keys())[1]+'='+list(cookie.values())[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# login & get cookie\n",
    "headers = {\n",
    "    'Connection': 'keep-alive',\n",
    "    'Origin': 'https://gab.com',\n",
    "    'User-Agent': ua,\n",
    "    'Referer': LOGIN_URL,\n",
    "    'Cookie': cookie\n",
    "}\n",
    "\n",
    "data = {'user[email]':EMAIL,'user[password]':PASSWORD,'authenticity_token':authenticity_token}\n",
    "time.sleep(3)\n",
    "rs = requests.session()\n",
    "log_in = rs.post('https://gab.com/auth/sign_in', headers = headers, data = data)\n",
    "cookie = rs.cookies.get_dict()\n",
    "cookie = list(cookie.keys())[0]+'='+list(cookie.values())[0]+';'+list(cookie.keys())[1]+'='+list(cookie.values())[1]+';'+list(cookie.keys())[2]+'='+list(cookie.values())[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# first: get target users\n",
    "\n",
    "## get target_lists from:\n",
    "- explore hot stories...\n",
    "- and compare with our watch_user_list already monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get data from explore page\n",
    "headers = {\n",
    "    'Connection': 'keep-alive',\n",
    "    'User-Agent': ua,\n",
    "    'Referer': url_base,\n",
    "    'Cookie':cookie\n",
    "}\n",
    "\n",
    "url = 'https://gab.com/api/v1/timelines/explore?page=1&sort_by=hot'\n",
    "response = rs.get(url,headers = headers)\n",
    "explore_post = parse_explore(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▉      | 7/18 [01:01<01:35,  8.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed: explore data ends from page: 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(2,20)):\n",
    "    try:\n",
    "        time.sleep(random.randint(3,8))\n",
    "        url = 'https://gab.com/api/v1/timelines/explore?page='+str(i)+'&sort_by=hot'\n",
    "        response = rs.get(url,headers = headers)\n",
    "        add_explore_post = parse_explore(response)\n",
    "        explore_post.extend(add_explore_post)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    if len(str(response.text)) < 5:\n",
    "        print('Completed: explore data ends from page:', i)\n",
    "        break\n",
    "target_users = list(set([post['account']['id'] for post in explore_post]))\n",
    "write_json(explore_post, 'explore_post'+date_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# second: get posts of target users "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare watching list\n",
    "# we can compare then with data from time difference(timedelta) less then let's say 10 days \n",
    "# (which means in 10 days we don't update same user's data)\n",
    "try:\n",
    "    with open('user_data/watch_user_list.json') as f:\n",
    "        watch_list = json.load(f)\n",
    "except:\n",
    "    watch_list = dict()\n",
    "\n",
    "watch_dates = list(watch_list.keys())\n",
    "days = datetime.timedelta(no_repeat_watch_days)\n",
    "old_date = today - days\n",
    "compare_dates = [d for d in watch_dates if (str(old_date)<=d)&(d<=str(today))]\n",
    "already_watch = []\n",
    "[already_watch.extend(watch_list[d]) for d in compare_dates]\n",
    "already_watch = list(set(already_watch))\n",
    "\n",
    "new_targets = [x for x in target_users if x not in already_watch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 1/5 [00:17<01:10, 17.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "headers = {\n",
    "    'Connection': 'keep-alive',\n",
    "    'User-Agent': ua,\n",
    "    'Referer': url_base,\n",
    "    'Accept':'application/json, text/plain, */*',\n",
    "#     'Accept-Encoding':'gzip, deflate, br',\n",
    "#     'Accept-Language':'en-US,en;q=0.9',\n",
    "    'Cookie':cookie\n",
    "}\n",
    "\n",
    "# get all user data\n",
    "dict_all_user = dict()\n",
    "path = 'user_data/'\n",
    "files = glob.glob(path+\"*\")\n",
    "new_name = path+'all_user_data'+date_str+'.json'\n",
    "try:\n",
    "    for uid in tqdm(new_targets):\n",
    "        dict_all_user[uid] = get_user_data(uid, rs, headers) \n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    if new_name in files:\n",
    "        t = time.localtime()\n",
    "        current_time = time.strftime(\"%H:%M:%S\", t)[:-3].replace(':', '')\n",
    "        write_json(dict_all_user, 'all_user_data'+date_str+'_'+current_time)\n",
    "    else:\n",
    "        write_json(dict_all_user, 'all_user_data'+date_str)\n",
    "\n",
    "if new_name in files:\n",
    "    t = time.localtime()\n",
    "    current_time = time.strftime(\"%H:%M:%S\", t)[:-3].replace(':', '')\n",
    "    write_json(dict_all_user, 'all_user_data'+date_str+'_'+current_time)\n",
    "else:\n",
    "    write_json(dict_all_user, 'all_user_data'+date_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add new users into new watch_list\n",
    "try:\n",
    "    watch_list[str(date.today())].extend(target_users)\n",
    "except:\n",
    "    watch_list[str(date.today())] = target_users\n",
    "    \n",
    "watch_list[str(date.today())] = list(set(watch_list[str(date.today())]))\n",
    "write_json(watch_list, 'watch_user_list')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
