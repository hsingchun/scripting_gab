{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import bs4\n",
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import urllib.parse\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "from datetime import date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'url_data/'\n",
    "user_agent = [\n",
    "    \"Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; SV1; AcooBrowser; .NET CLR 1.1.4322; .NET CLR 2.0.50727)\",\n",
    "    \"Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 6.0; Acoo Browser; SLCC1; .NET CLR 2.0.50727; Media Center PC 5.0; .NET CLR 3.0.04506)\",\n",
    "    \"Mozilla/4.0 (compatible; MSIE 7.0; AOL 9.5; AOLBuild 4337.35; Windows NT 5.1; .NET CLR 1.1.4322; .NET CLR 2.0.50727)\",\n",
    "    \"Mozilla/5.0 (Windows; U; MSIE 9.0; Windows NT 9.0; en-US)\",\n",
    "    \"Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; Win64; x64; Trident/5.0; .NET CLR 3.5.30729; .NET CLR 3.0.30729; .NET CLR 2.0.50727; Media Center PC 6.0)\",\n",
    "    \"Mozilla/5.0 (compatible; MSIE 8.0; Windows NT 6.0; Trident/4.0; WOW64; Trident/4.0; SLCC2; .NET CLR 2.0.50727; .NET CLR 3.5.30729; .NET CLR 3.0.30729; .NET CLR 1.0.3705; .NET CLR 1.1.4322)\",\n",
    "    \"Mozilla/4.0 (compatible; MSIE 7.0b; Windows NT 5.2; .NET CLR 1.1.4322; .NET CLR 2.0.50727; InfoPath.2; .NET CLR 3.0.04506.30)\",\n",
    "    \"Mozilla/5.0 (Windows; U; Windows NT 5.1; zh-CN) AppleWebKit/523.15 (KHTML, like Gecko, Safari/419.3) Arora/0.3 (Change: 287 c9dfb30)\",\n",
    "    \"Mozilla/5.0 (X11; U; Linux; en-US) AppleWebKit/527+ (KHTML, like Gecko, Safari/419.3) Arora/0.6\",\n",
    "    \"Mozilla/5.0 (Windows; U; Windows NT 5.1; en-US; rv:1.8.1.2pre) Gecko/20070215 K-Ninja/2.1.1\",\n",
    "    \"Mozilla/5.0 (Windows; U; Windows NT 5.1; zh-CN; rv:1.9) Gecko/20080705 Firefox/3.0 Kapiko/3.0\",\n",
    "    \"Mozilla/5.0 (X11; Linux i686; U;) Gecko/20070322 Kazehakase/0.4.5\",\n",
    "    \"Mozilla/5.0 (X11; U; Linux i686; en-US; rv:1.9.0.8) Gecko Fedora/1.9.0.8-1.fc10 Kazehakase/0.5.6\",\n",
    "    \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/535.11 (KHTML, like Gecko) Chrome/17.0.963.56 Safari/535.11\",\n",
    "    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_7_3) AppleWebKit/535.20 (KHTML, like Gecko) Chrome/19.0.1036.7 Safari/535.20\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_comment_dict(comment_container):\n",
    "    try:\n",
    "        dict_comment = dict()\n",
    "        dict_comment['author_id '] = comment_container.get('data-author-id')\n",
    "        dict_comment['comment_id '] = comment_container.get('data-comment-id')\n",
    "        dict_comment['comment'] = comment_container.find(attrs={\"class\": \"comment-body\"}).text.split('\\n')[0]\n",
    "        return dict_comment\n",
    "    except Exception as e:\n",
    "        print('comment_container = ', comment_container)\n",
    "        print(e)\n",
    "        \n",
    "def request_session(url, user_agent):\n",
    "    rs = requests.session()\n",
    "    try:\n",
    "        ua = random.choice(user_agent)\n",
    "        html = rs.get(url, headers = {'User-Agent':ua, 'Connection':'close'})\n",
    "        return html\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        \n",
    "def get_first_10_data(html, surfing_url):\n",
    "    obj = bs4.BeautifulSoup(html.text, 'lxml')\n",
    "    comment_containers = (obj.find_all(attrs={\"class\": \"comment-container\"}))\n",
    "    li_dict_comment = [get_comment_dict(com) for com in comment_containers]\n",
    "    \n",
    "    dict_all_comm = dict()\n",
    "    dict_all_comm['last_collected_date '] = str(date.today())\n",
    "    dict_all_comm['web_url'] = surfing_url\n",
    "    commenturl_id = str(obj.find(attrs={\"class\": \"col-auto\"})).split('\"><div')[1]\n",
    "    commenturl_id = commenturl_id.split('=\"')[1]\n",
    "    dict_all_comm['commenturl_id'] = commenturl_id\n",
    "    dict_all_comm['n_comments'] = int((obj.find(attrs={\"class\": \"h4\"}).text.split(' ')[0]).replace(',',''))\n",
    "    dict_all_comm['comments_info'] = li_dict_comment\n",
    "    return dict_all_comm\n",
    "\n",
    "def get_first_page(surfing_url, user_agent):\n",
    "    str_surfing_url = urllib.parse.quote_plus(surfing_url)\n",
    "    #first page: top 10 comments\n",
    "    url_base = 'https://dissenter.com/discussion/begin-extension?url='\n",
    "    url = url_base+str_surfing_url \n",
    "    html = request_session(url, user_agent)\n",
    "    dict_all_comm = get_first_10_data(html, surfing_url)\n",
    "    return dict_all_comm\n",
    "\n",
    "def get_rest_pages(dict_all_comm, user_agent):\n",
    "    surfing_url = dict_all_comm['web_url']\n",
    "    commenturl_id = dict_all_comm['commenturl_id']\n",
    "    #start from page2: load more comments\n",
    "    n_page = math.ceil(dict_all_comm['n_comments']/10)\n",
    "    for page_i in tqdm(range(2,n_page+1)):\n",
    "        url_base_2 = 'https://dissenter.com/comment?url='\n",
    "        url2_add = surfing_url+'&v=begin&uid='+commenturl_id+'&s=top&p='+str(page_i)+'&cpp=10&more=1'\n",
    "        url2 = url_base_2+url2_add\n",
    "\n",
    "        time.sleep(random.randint(4,10))\n",
    "        try:\n",
    "            html2 = request_session(url2, user_agent)\n",
    "            obj2 = bs4.BeautifulSoup(html2.text, 'lxml')\n",
    "            comment_containers = (obj2.find_all(attrs={\"class\": \"comment-container\"}))\n",
    "            li_dict_comment = [get_comment_dict(com) for com in comment_containers]\n",
    "            dict_all_comm['comments_info'].extend(li_dict_comment)\n",
    "        except Exception as e:\n",
    "            print('page_i = ', page_i)\n",
    "            print(e)\n",
    "        if len(obj2.find_all('div')) == 1:\n",
    "            print('Completed: comment data ends from page:', page_i)\n",
    "            return dict_all_comm\n",
    "            break\n",
    "    return dict_all_comm\n",
    "\n",
    "def write_json(dict_all_comm, surfing_url, path):\n",
    "    file_name = surfing_url.split('://')[1]\n",
    "    file_name = file_name.split('.com/')[0]\n",
    "    file_name = file_name.replace('.','_')\n",
    "    file_name = file_name.replace('/','_')\n",
    "    with open(path+file_name+'.json', 'w') as f:    \n",
    "        json.dump(dict_all_comm, f)\n",
    "        \n",
    "def get_file_one_url(surfing_url, user_agent, path):\n",
    "    surfing_url = surfing_url.split('?')[0]\n",
    "    dict_all_comm = get_first_page(surfing_url, user_agent)\n",
    "    dict_all_comm = get_rest_pages(dict_all_comm, user_agent)\n",
    "    try:\n",
    "        file_name = surfing_url.split('://')[1]\n",
    "        file_name = file_name.split('.com/')[0]\n",
    "        file_name = file_name.replace('.','_')\n",
    "        file_name = file_name.replace('/','_')\n",
    "        with open(path+file_name+'.json') as f:\n",
    "            old_file = json.load(f)\n",
    "            if type(old_file) == dict:\n",
    "                all_res = []\n",
    "                all_res.append(old_file)\n",
    "                all_res.append(dict_all_comm)\n",
    "                write_json(all_res, surfing_url, path)\n",
    "            else:\n",
    "                dict_all_comm.append(dict_all_comm)\n",
    "                write_json(dict_all_comm, surfing_url, path)\n",
    "    except:\n",
    "        write_json(dict_all_comm, surfing_url, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We provide two ways to use this url tracker,       do you what to (A) input them one by one, or (B) have an url list in json file format?      please input A or B:\n",
      "b\n",
      "please make sure your file is put under the path setted above,     and input your file name(without \".json\"):\n",
      "url_targets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      " 82%|████████▏ | 31/38 [04:06<00:55,  7.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed: comment data ends from page: 33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 63%|██████▎   | 122/193 [17:51<10:23,  8.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed: comment data ends from page: 124\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 65%|██████▍   | 11/17 [01:34<00:51,  8.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed: comment data ends from page: 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 58%|█████▊    | 11/19 [01:33<01:08,  8.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed: comment data ends from page: 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 71%|███████▏  | 10/14 [01:46<00:42, 10.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed: comment data ends from page: 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "# input\n",
    "print('We provide two ways to use this url tracker, \\\n",
    "      do you what to (A) input them one by one, or (B) have an url list in json file format?\\\n",
    "      please input A or B:')\n",
    "way = input()\n",
    "if way in (['A','a']):\n",
    "    print('Please input the URL you want to trace:  (ex. https://www.facebook.com/)')\n",
    "    surfing_url = input()\n",
    "    get_file_one_url(surfing_url, user_agent, path)\n",
    "elif way in(['B','b']):\n",
    "    print('please make sure your file is put under the path setted above, \\\n",
    "    and input your file name(without \".json\"):')\n",
    "    f_name = input()\n",
    "    with open(path+f_name+'.json') as f:    \n",
    "        url_targets = json.load(f)\n",
    "        [get_file_one_url(url, user_agent, path) for url in url_targets]\n",
    "else:\n",
    "    print('please input A or B....')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
