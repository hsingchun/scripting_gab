{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import bs4\n",
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import urllib.parse\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "from datetime import date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 有問題：ＩＰ使用方式應該不對"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'url_data/'\n",
    "user_agent = [\n",
    "    \"Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; SV1; AcooBrowser; .NET CLR 1.1.4322; .NET CLR 2.0.50727)\",\n",
    "    \"Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 6.0; Acoo Browser; SLCC1; .NET CLR 2.0.50727; Media Center PC 5.0; .NET CLR 3.0.04506)\",\n",
    "    \"Mozilla/4.0 (compatible; MSIE 7.0; AOL 9.5; AOLBuild 4337.35; Windows NT 5.1; .NET CLR 1.1.4322; .NET CLR 2.0.50727)\",\n",
    "    \"Mozilla/5.0 (Windows; U; MSIE 9.0; Windows NT 9.0; en-US)\",\n",
    "    \"Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; Win64; x64; Trident/5.0; .NET CLR 3.5.30729; .NET CLR 3.0.30729; .NET CLR 2.0.50727; Media Center PC 6.0)\",\n",
    "    \"Mozilla/5.0 (compatible; MSIE 8.0; Windows NT 6.0; Trident/4.0; WOW64; Trident/4.0; SLCC2; .NET CLR 2.0.50727; .NET CLR 3.5.30729; .NET CLR 3.0.30729; .NET CLR 1.0.3705; .NET CLR 1.1.4322)\",\n",
    "    \"Mozilla/4.0 (compatible; MSIE 7.0b; Windows NT 5.2; .NET CLR 1.1.4322; .NET CLR 2.0.50727; InfoPath.2; .NET CLR 3.0.04506.30)\",\n",
    "    \"Mozilla/5.0 (Windows; U; Windows NT 5.1; zh-CN) AppleWebKit/523.15 (KHTML, like Gecko, Safari/419.3) Arora/0.3 (Change: 287 c9dfb30)\",\n",
    "    \"Mozilla/5.0 (X11; U; Linux; en-US) AppleWebKit/527+ (KHTML, like Gecko, Safari/419.3) Arora/0.6\",\n",
    "    \"Mozilla/5.0 (Windows; U; Windows NT 5.1; en-US; rv:1.8.1.2pre) Gecko/20070215 K-Ninja/2.1.1\",\n",
    "    \"Mozilla/5.0 (Windows; U; Windows NT 5.1; zh-CN; rv:1.9) Gecko/20080705 Firefox/3.0 Kapiko/3.0\",\n",
    "    \"Mozilla/5.0 (X11; Linux i686; U;) Gecko/20070322 Kazehakase/0.4.5\",\n",
    "    \"Mozilla/5.0 (X11; U; Linux i686; en-US; rv:1.9.0.8) Gecko Fedora/1.9.0.8-1.fc10 Kazehakase/0.5.6\",\n",
    "    \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/535.11 (KHTML, like Gecko) Chrome/17.0.963.56 Safari/535.11\",\n",
    "    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_7_3) AppleWebKit/535.20 (KHTML, like Gecko) Chrome/19.0.1036.7 Safari/535.20\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_IPs = 30\n",
    "import asyncio\n",
    "from proxybroker import Broker\n",
    "list_proxy = []\n",
    "async def view_proxy(proxies_queue): \n",
    "    while True: \n",
    "        proxy = await proxies_queue.get() \n",
    "        p = str(proxy).split('] ')[1]\n",
    "        p = p.split(\">\")[0]\n",
    "        list_proxy.append(p)\n",
    "        if proxy is None:\n",
    "            break \n",
    "        print(proxy)\n",
    "\n",
    "proxies_queue = asyncio.Queue()\n",
    "broker = Broker(proxies_queue)\n",
    "# tasks = asyncio.gather(broker.grab(limit=300), view_proxy(proxies_queue))\n",
    "tasks = asyncio.gather(broker.find(types=['HTTP', 'HTTPS'], limit=N_IPs), view_proxy(proxies_queue))\n",
    "loop = asyncio.get_event_loop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_comment_dict(comment_container):\n",
    "    try:\n",
    "        dict_comment = dict()\n",
    "        dict_comment['last_collected_date '] = str(date.today())\n",
    "        dict_comment['author_id '] = comment_container.get('data-author-id')\n",
    "        dict_comment['comment_id '] = comment_container.get('data-comment-id')\n",
    "        dict_comment['comment'] = comment_container.find(attrs={\"class\": \"comment-body\"}).text.split('\\n')[0]\n",
    "        return dict_comment\n",
    "    except Exception as e:\n",
    "        print('comment_container = ', comment_container)\n",
    "        print(e)\n",
    "        \n",
    "def request_session(url, user_agent, ip):\n",
    "    rs = requests.session()\n",
    "    try:\n",
    "        ua = random.choice(user_agent)\n",
    "        html = rs.get(url, headers = {'User-Agent':ua, \n",
    "                                      'Connection':'close'},\n",
    "                     proxies=ip)\n",
    "        return html\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        \n",
    "def get_first_10_data(html):\n",
    "    obj = bs4.BeautifulSoup(html.text, 'lxml')\n",
    "    comment_containers = (obj.find_all(attrs={\"class\": \"comment-container\"}))\n",
    "    li_dict_comment = [get_comment_dict(com) for com in comment_containers]\n",
    "    \n",
    "    dict_all_comm = dict()\n",
    "    dict_all_comm['web_url'] = surfing_url\n",
    "    commenturl_id = str(obj.find(attrs={\"class\": \"col-auto\"})).split('\"><div')[1]\n",
    "    commenturl_id = commenturl_id.split('=\"')[1]\n",
    "    dict_all_comm['commenturl_id'] = commenturl_id\n",
    "    dict_all_comm['n_comments'] = int((obj.find(attrs={\"class\": \"h4\"}).text.split(' ')[0]).replace(',',''))\n",
    "    dict_all_comm['comments_info'] = li_dict_comment\n",
    "    return dict_all_comm\n",
    "\n",
    "def get_first_page(surfing_url, user_agent, proxy):\n",
    "    str_surfing_url = urllib.parse.quote_plus(surfing_url)\n",
    "    #first page: top 10 comments\n",
    "    url_base = 'https://dissenter.com/discussion/begin-extension?url='\n",
    "    url = url_base+str_surfing_url \n",
    "    ip = random.choice(proxy)\n",
    "    html = request_session(url, user_agent, ip)\n",
    "    dict_all_comm = get_first_10_data(html)\n",
    "    return dict_all_comm, ip\n",
    "\n",
    "def get_rest_pages(dict_all_comm, user_agent, ip):\n",
    "    surfing_url = dict_all_comm['web_url']\n",
    "    commenturl_id = dict_all_comm['commenturl_id']\n",
    "    #start from page2: load more comments\n",
    "    n_page = math.ceil(dict_all_comm['n_comments']/10)\n",
    "    for page_i in tqdm(range(2,n_page+1)):\n",
    "        url_base_2 = 'https://dissenter.com/comment?url='\n",
    "        url2_add = surfing_url+'&v=begin&uid='+commenturl_id+'&s=top&p='+str(page_i)+'&cpp=10&more=1'\n",
    "        url2 = url_base_2+url2_add\n",
    "\n",
    "        time.sleep(random.randint(4,10))\n",
    "        try:\n",
    "            html2 = request_session(url2, user_agent, ip)\n",
    "            obj2 = bs4.BeautifulSoup(html2.text, 'lxml')\n",
    "            comment_containers = (obj2.find_all(attrs={\"class\": \"comment-container\"}))\n",
    "            li_dict_comment = [get_comment_dict(com) for com in comment_containers]\n",
    "            dict_all_comm['comments_info'].extend(li_dict_comment)\n",
    "        except Exception as e:\n",
    "            print('page_i = ', page_i)\n",
    "            print(e)\n",
    "        if len(obj2.find_all('div')) == 1:\n",
    "            print('Completed: comment data ends from page:', page_i)\n",
    "            return dict_all_comm\n",
    "            break\n",
    "    return dict_all_comm\n",
    "\n",
    "def write_json(dict_all_comm, surfing_url, path):\n",
    "    file_name = surfing_url.split('://')[1]\n",
    "    file_name = file_name.split('.com/')[0]\n",
    "    file_name = file_name.replace('.','_')\n",
    "    file_name = file_name.replace('/','_')\n",
    "    with open(path+file_name+'.json', 'w') as f:    \n",
    "        json.dump(dict_all_comm, f)\n",
    "        \n",
    "def get_file_one_url(surfing_url, user_agent, path, proxy):\n",
    "    surfing_url = surfing_url.split('?')[0]\n",
    "    dict_all_comm, ip = get_first_page(surfing_url, user_agent, proxy)\n",
    "    dict_all_comm = get_rest_pages(dict_all_comm, user_agent, ip)\n",
    "    try:\n",
    "        file_name = surfing_url.split('://')[1]\n",
    "        file_name = file_name.split('.com/')[0]\n",
    "        file_name = file_name.replace('.','_')\n",
    "        file_name = file_name.replace('/','_')\n",
    "        with open(path+file_name+'.json') as f:\n",
    "            old_file = json.load(f)\n",
    "            if type(old_file) == dict:\n",
    "                all_res = []\n",
    "                all_res.append(old_file)\n",
    "                all_res.append(dict_all_comm)\n",
    "                write_json(all_res, surfing_url, path)\n",
    "            else:\n",
    "                dict_all_comm.append(dict_all_comm)\n",
    "                write_json(dict_all_comm, surfing_url, path)\n",
    "    except:\n",
    "        write_json(dict_all_comm, surfing_url, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We provide two ways to use this url tracker,       do you what to (A) input them one by one, or (B) have an url list in json file format?      please input A or B:\n",
      "a\n",
      "Please input the URL you want to trace:  (ex. https://www.facebook.com/)\n",
      "https://en.wikipedia.org/wiki/United_States\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "Cannot choose from an empty sequence",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-774b782910da>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Please input the URL you want to trace:  (ex. https://www.facebook.com/)'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0msurfing_url\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mget_file_one_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msurfing_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muser_agent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproxy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32melif\u001b[0m \u001b[0mway\u001b[0m \u001b[0;32min\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'B'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'b'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     print('please make sure your file is put under the path setted above, \\\n",
      "\u001b[0;32m<ipython-input-4-988c192f6a6e>\u001b[0m in \u001b[0;36mget_file_one_url\u001b[0;34m(surfing_url, user_agent, path, proxy)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_file_one_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msurfing_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muser_agent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproxy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0msurfing_url\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msurfing_url\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'?'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m     \u001b[0mdict_all_comm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mip\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_first_page\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msurfing_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muser_agent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproxy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m     \u001b[0mdict_all_comm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_rest_pages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdict_all_comm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muser_agent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-988c192f6a6e>\u001b[0m in \u001b[0;36mget_first_page\u001b[0;34m(surfing_url, user_agent, proxy)\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0murl_base\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'https://dissenter.com/discussion/begin-extension?url='\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0murl_base\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr_surfing_url\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m     \u001b[0mip\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproxy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m     \u001b[0mhtml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequest_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muser_agent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0mdict_all_comm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_first_10_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhtml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python/3.7.5/Frameworks/Python.framework/Versions/3.7/lib/python3.7/random.py\u001b[0m in \u001b[0;36mchoice\u001b[0;34m(self, seq)\u001b[0m\n\u001b[1;32m    259\u001b[0m             \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_randbelow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 261\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Cannot choose from an empty sequence'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    262\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mseq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: Cannot choose from an empty sequence"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Proxy US 0.24s [HTTP: High] 47.242.133.230:8083>\n",
      "<Proxy US 0.30s [HTTP: Anonymous, HTTPS] 20.195.17.90:3128>\n",
      "<Proxy US 0.37s [HTTP: Anonymous, HTTPS] 159.89.221.73:3128>\n",
      "<Proxy MY 0.37s [HTTP: High, HTTPS] 219.92.3.149:8080>\n",
      "<Proxy US 0.41s [HTTP: Anonymous, HTTPS] 34.94.40.85:80>\n",
      "<Proxy ID 0.65s [HTTP: High] 118.99.100.131:8080>\n",
      "<Proxy US 0.25s [HTTP: High] 8.210.175.82:8083>\n",
      "<Proxy SG 0.26s [HTTP: High] 128.199.202.122:3128>\n",
      "<Proxy US 0.29s [HTTP: High] 191.96.42.80:8080>\n",
      "<Proxy UZ 0.75s [HTTP: High] 213.230.97.10:3128>\n",
      "<Proxy CA 0.30s [HTTP: High] 159.203.61.169:8080>\n",
      "<Proxy UZ 0.77s [HTTP: High] 84.54.82.234:3128>\n",
      "<Proxy US 0.33s [HTTP: High] 198.144.166.51:80>\n",
      "<Proxy JP 0.23s [HTTPS] 161.202.226.194:80>\n",
      "<Proxy IN 0.39s [HTTP: High] 139.59.1.14:8080>\n",
      "<Proxy CA 0.56s [HTTP: High, HTTPS] 144.217.101.245:3129>\n",
      "<Proxy DE 0.39s [HTTP: High] 192.109.165.136:80>\n",
      "<Proxy SG 0.39s [HTTP: High] 191.101.39.40:80>\n",
      "<Proxy DE 0.39s [HTTP: High] 192.109.165.50:80>\n",
      "<Proxy SG 0.39s [HTTP: High] 191.101.39.178:80>\n",
      "<Proxy IN 0.39s [HTTP: Anonymous] 122.186.181.124:80>\n",
      "<Proxy MX 0.57s [HTTPS] 169.57.1.85:80>\n",
      "<Proxy MX 0.36s [HTTPS] 169.57.1.84:8123>\n",
      "<Proxy -- 0.74s [HTTP: High, HTTPS] 103.158.69.178:3128>\n",
      "<Proxy US 0.29s [HTTP: Anonymous] 207.157.25.41:80>\n",
      "<Proxy AU 0.81s [HTTP: Anonymous] 203.33.113.65:80>\n",
      "<Proxy VN 0.91s [HTTP: Anonymous, HTTPS] 14.225.5.68:80>\n",
      "<Proxy GB 0.98s [HTTP: High] 167.172.110.165:9090>\n",
      "<Proxy AU 1.07s [HTTP: Anonymous] 203.33.113.183:80>\n",
      "<Proxy GB 1.06s [HTTP: High, HTTPS] 51.79.157.202:443>\n"
     ]
    }
   ],
   "source": [
    "# input\n",
    "print('We provide two ways to use this url tracker, \\\n",
    "      do you what to (A) input them one by one, or (B) have an url list in json file format?\\\n",
    "      please input A or B:')\n",
    "way = input()\n",
    "if way in (['A','a']):\n",
    "    print('Please input the URL you want to trace:  (ex. https://www.facebook.com/)')\n",
    "    surfing_url = input()\n",
    "    get_file_one_url(surfing_url, user_agent, path, proxy)\n",
    "elif way in(['B','b']):\n",
    "    print('please make sure your file is put under the path setted above, \\\n",
    "    and input your file name(without \".json\"):')\n",
    "    f_name = input()\n",
    "    with open(path+f_name+'.json') as f:    \n",
    "        url_targets = json.load(f)\n",
    "        [get_file_one_url(url, user_agent, path, proxy) for url in url_targets]\n",
    "else:\n",
    "    print('please input A or B....')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
